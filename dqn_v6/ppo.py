import gymnasium as gym
import numpy as np, torch, random, time, torchvision.transforms as T
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from torch.utils.tensorboard import SummaryWriter

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. ê³µí†µ ì„¤ì • (DQNê³¼ ë™ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEED, RESIZE = 42, (84, 84)
NUM_EPISODES, MAX_STEPS = 1_000, 1_000
TOTAL_TIMESTEPS = NUM_EPISODES * MAX_STEPS      # 1,000,000
LR = 5e-5                                        # DQNê³¼ ë™ì¼
BATCH_SIZE = 128
LOG_ROOT = f"runs/ppo_carracing_{int(time.time())}"   # DQN í´ë”ëª… í˜•ì‹
device = "cuda" if torch.cuda.is_available() else "cpu"

random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.benchmark = True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. í™˜ê²½ ë˜í¼ (DQNê³¼ ë™ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class CarRacingDiscrete(gym.Wrapper):
    ACTIONS = [
        np.array([-1.,0.,0.],np.float32), np.array([1.,0.,0.],np.float32),
        np.array([0.,1.,0.],np.float32),  np.array([0.,0.,0.8],np.float32),
        np.array([-1.,1.,0.],np.float32), np.array([1.,1.,0.],np.float32),
        np.array([0.,0.,0.],np.float32),
    ]
    def __init__(self, resize=RESIZE):
        super().__init__(gym.make("CarRacing-v3", render_mode="rgb_array"))
        self.action_space = gym.spaces.Discrete(len(self.ACTIONS))
        self.observation_space = gym.spaces.Box(0,255,(resize[0],resize[1],1),np.uint8)
        self.tf = T.Compose([T.ToPILImage(),T.Grayscale(1),
                             T.Resize(resize,T.InterpolationMode.BILINEAR),T.ToTensor()])
    def _p(self,f): return (self.tf(f)*255).byte().permute(1,2,0).numpy()
    def reset(self,**kw): obs,info=self.env.reset(**kw); return self._p(obs),info
    def step(self,a): obs,r,t,tr,i=self.env.step(self.ACTIONS[a]); return self._p(obs),r,t,tr,i

class ChannelsFirst(gym.ObservationWrapper):
    def __init__(self,env):
        super().__init__(env)
        h,w,c = env.observation_space.shape
        self.observation_space = gym.spaces.Box(0,255,(c,h,w),np.uint8)
    def observation(self,obs): return np.transpose(obs,(2,0,1))

def make_env():
    def _init():
        env = CarRacingDiscrete()
        env = ChannelsFirst(env)
        env = Monitor(env)          # episode reward/length ê¸°ë¡
        env.reset(seed=SEED)
        return env
    return _init

vec_env = DummyVecEnv([make_env()])
vec_env = VecFrameStack(vec_env,n_stack=4)       # (4,C,H,W)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. SummaryWriter ì´ˆê¸°í™” (DQNê³¼ ë™ì¼í•œ êµ¬ì¡°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
writer = SummaryWriter(log_dir=f"{LOG_ROOT}/ppo")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ì½œë°±: DQNê³¼ ì™„ì „íˆ ë™ì¼í•œ íƒœê·¸ë¡œ ê¸°ë¡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DQNCompatibleCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.ep_ret, self.ep_len = 0.0, 0
        self.ep_count = 0
        self.loss_buffer = []  # PPO ì†ì‹¤ ê¸°ë¡ìš©
        
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œ ì§„í–‰ ì¤‘ ë¦¬ì›Œë“œ/ìŠ¤í… ëˆ„ì 
        self.ep_ret += self.locals["rewards"][0]
        self.ep_len += 1
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ê¸°ë¡
        if self.locals["dones"][0]:
            self.ep_count += 1
            
            # â˜… DQNê³¼ ë™ì¼í•œ íƒœê·¸ ì‚¬ìš©
            writer.add_scalar("Episode/Reward", self.ep_ret, self.ep_count)
            writer.add_scalar("Episode/Steps", self.ep_len, self.ep_count)
            writer.add_scalar("Episode/Learning_Rate", 
                            self.model.lr_schedule(self.model._current_progress_remaining), 
                            self.ep_count)
            
            # PPO íŠ¹í™” ì •ë³´ë„ DQN ìŠ¤íƒ€ì¼ë¡œ ê¸°ë¡
            if hasattr(self.model, 'logger') and self.model.logger.name_to_value:
                # PPO ì†ì‹¤ ì •ë³´ (DQN Loss íƒœê·¸ì™€ ê°™ì€ í˜•ì‹)
                if 'train/policy_loss' in self.model.logger.name_to_value:
                    policy_loss = self.model.logger.name_to_value['train/policy_loss']
                    writer.add_scalar("Episode/Loss", policy_loss, self.ep_count)
                
                # ì¶”ê°€ì ì¸ PPO ë©”íŠ¸ë¦­ë“¤ (DQNê³¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì ‘ë‘ì‚¬ ìœ ì§€)
                if 'train/value_loss' in self.model.logger.name_to_value:
                    value_loss = self.model.logger.name_to_value['train/value_loss']
                    writer.add_scalar("Episode/Value_Loss", value_loss, self.ep_count)
                
                if 'train/entropy_loss' in self.model.logger.name_to_value:
                    entropy_loss = self.model.logger.name_to_value['train/entropy_loss']
                    writer.add_scalar("Episode/Entropy_Loss", entropy_loss, self.ep_count)
                
                # í´ë¦¬í•‘ ê´€ë ¨ ì •ë³´
                if 'train/clip_fraction' in self.model.logger.name_to_value:
                    clip_frac = self.model.logger.name_to_value['train/clip_fraction']
                    writer.add_scalar("Episode/Clip_Fraction", clip_frac, self.ep_count)
            
            # ì—í”¼ì†Œë“œë³„ ì¶œë ¥ (DQN ìŠ¤íƒ€ì¼)
            print(f"Ep {self.ep_count:4d} | Reward {self.ep_ret:7.1f} | Steps {self.ep_len:4d} | LR {self.model.lr_schedule(self.model._current_progress_remaining):.2e}")
            
            # ë¦¬ì…‹
            self.ep_ret, self.ep_len = 0.0, 0
            
        return True
    
    def _on_rollout_end(self) -> None:
        """PPO ë¡¤ì•„ì›ƒ ì¢…ë£Œ ì‹œ ì¶”ê°€ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if hasattr(self.model, 'logger') and self.model.logger.name_to_value:
            # í˜„ì¬ ìŠ¤í… ìˆ˜ ê³„ì‚°
            current_timestep = self.model.num_timesteps
            
            # í‰ê·  ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ (ê°€ëŠ¥í•œ ê²½ìš°)
            if hasattr(self.model.env, 'get_attr'):
                try:
                    episode_rewards = self.model.env.get_attr('episode_rewards')
                    if episode_rewards and episode_rewards[0]:
                        avg_reward = np.mean(episode_rewards[0][-10:])  # ìµœê·¼ 10ê°œ í‰ê· 
                        writer.add_scalar("Training/Average_Reward", avg_reward, current_timestep)
                except:
                    pass
    
    def _on_training_end(self) -> None:
        writer.close()
        print(f"âœ… PPO training finished. Total episodes: {self.ep_count}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. PPO ëª¨ë¸ (DQN í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì •ë ¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = PPO(
    "CnnPolicy",
    vec_env,
    learning_rate=LR,                # DQNê³¼ ë™ì¼
    batch_size=BATCH_SIZE,           # DQNê³¼ ë™ì¼
    n_steps=1024,                    # DQNì˜ replay bufferì™€ ìœ ì‚¬í•œ ê²½í—˜ ìˆ˜ì§‘
    n_epochs=4,                      # PPO íŠ¹í™”
    gamma=0.99,                      # DQNê³¼ ë™ì¼
    clip_range=0.2,                  # PPO íŠ¹í™”
    gae_lambda=0.95,                 # PPO íŠ¹í™”
    vf_coef=0.5,                     # value function ê³„ìˆ˜
    ent_coef=0.01,                   # entropy ê³„ìˆ˜
    tensorboard_log=None,            # ìˆ˜ë™ìœ¼ë¡œ ê´€ë¦¬
    seed=SEED,
    device=device,
    verbose=0,                       # ì½œë°±ì—ì„œ ì¶œë ¥ ê´€ë¦¬
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. í•™ìŠµ ì‹œì‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"ğŸš€ PPO training started")
print(f"   Total timesteps: {TOTAL_TIMESTEPS:,}")
print(f"   Log directory: {LOG_ROOT}")
print(f"   Device: {device}")
print(f"   Learning rate: {LR}")
print(f"   Batch size: {BATCH_SIZE}")
print("-" * 60)

model.learn(
    total_timesteps=TOTAL_TIMESTEPS, 
    callback=DQNCompatibleCallback(),
    progress_bar=True
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ëª¨ë¸ ì €ì¥ ë° ì •ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.save("ppo_carracing_discrete_final_dqn_compatible")
vec_env.close()

print("-" * 60)
print("âœ… PPO training completed!")
print(f"ğŸ“Š Model saved as: ppo_carracing_discrete_final_dqn_compatible")
print(f"ğŸ“ˆ TensorBoard logs: {LOG_ROOT}")
print(f"ğŸ’¡ To compare with DQN: tensorboard --logdir runs/")