import gymnasium as gym
import math
import random
import numpy as np
from collections import namedtuple, deque
from itertools import count
import time

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T

from PIL import Image
from torch.utils.tensorboard import SummaryWriter
from my_plot import my_plot


random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# CuDNNì´ í•©ì„±ê³± ì—°ì‚°ì— ëŒ€í•´ ìµœì í™”ëœ ì•Œê³ ë¦¬ì¦˜ì„ ìë™ ì„ íƒí•˜ë„ë¡ ì„¤ì •
torch.backends.cudnn.benchmark = True

# ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì„¤ì • (ë©€í‹°-GPU í™˜ê²½ì´ë¼ë©´ "cuda"ë¡œ ì§€ì •í•˜ë©´ DataParallelì´ ìë™ ë¶„ë°°í•´ì¤ë‹ˆë‹¤)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ“¢ Using device: {device}")  # í•™ìŠµ ì‹œì‘ ì‹œ GPU/CPU í™•ì¸ìš© í”„ë¦°íŠ¸

class CarRacingDiscrete(gym.Wrapper):
    # ê¸°ë³¸ 7ê°œ ì•¡ì…˜: ì¢Œ, ìš°, ê°€ì†, ë¸Œë ˆì´í¬, ê°€ì†+ì¢Œ, ê°€ì†+ìš°, ë¬´ë™ì‘
    ACTIONS = [
        np.array([-1.0, 0.0, 0.0], dtype=np.float32),  # steer left
        np.array([1.0, 0.0, 0.0], dtype=np.float32),   # steer right
        np.array([0.0, 1.0, 0.0], dtype=np.float32),   # gas
        np.array([0.0, 0.0, 0.8], dtype=np.float32),   # brake
        np.array([-1.0, 1.0, 0.0], dtype=np.float32),  # gas + left
        np.array([1.0, 1.0, 0.0], dtype=np.float32),   # gas + right
        np.array([0.0, 0.0, 0.0], dtype=np.float32),   # no-op
    ]

    def __init__(self, resize=(84, 84)):
        env = gym.make("CarRacing-v3", render_mode="rgb_array")
        super().__init__(env)
        self.action_space = gym.spaces.Discrete(len(self.ACTIONS))
        self.observation_space = gym.spaces.Box(0, 255, (resize[0], resize[1], 1), dtype=np.uint8)
        self.transform = T.Compose([
            T.ToPILImage(),
            T.Grayscale(num_output_channels=1),
            T.Resize(resize, interpolation=T.InterpolationMode.BILINEAR),
            T.ToTensor(),
        ])

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(self.ACTIONS[action])
        obs = self._process_frame(obs)
        return obs, reward, terminated, truncated, info

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        obs = self._process_frame(obs)
        return obs, info

    def _process_frame(self, frame):
        # [H, W, 3] â†’ [1, H, W] í‘ë°±ìœ¼ë¡œ, ì´í›„ [H, W]ë¡œ ì¶•ì†Œ â†’ uint8
        frame = self.transform(frame).squeeze(0)  # [H, W]
        frame = (frame * 255).byte()              # uint8
        return frame.numpy()                      # numpy array

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class ReplayMemory(object):
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        # args: (state_tensor, action_tensor, next_state_tensor, reward_tensor)
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. DQN ë„¤íŠ¸ì›Œí¬ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DQN(nn.Module):
    def __init__(self, h, w, outputs):
        super().__init__()
        # ì…ë ¥ ì±„ë„ = 4 (í”„ë ˆì„ ìŠ¤íƒ)
        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)

        def conv2d_size_out(size, kernel_size, stride):
            return (size - (kernel_size - 1) - 1) // stride + 1

        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2), 3, 1)
        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h, 8, 4), 4, 2), 3, 1)
        linear_input_size = convw * convh * 64

        self.fc   = nn.Linear(linear_input_size, 512)
        self.head = nn.Linear(512, outputs)

    def forward(self, x):
        # x: [batch_size, 4, H, W], uint8 í˜•íƒœ â†’ ì‹¤ìˆ˜í˜•ìœ¼ë¡œ ì •ê·œí™”
        x = x / 255.0
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = torch.flatten(x, 1)
        x = F.relu(self.fc(x))
        return self.head(x)  # [batch_size, outputs]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° í™˜ê²½ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BATCH_SIZE            = 128
GAMMA                 = 0.99
EPS_START             = 1.0
EPS_END               = 0.05
EPS_DECAY             = 50_000  # íƒí—˜ë¥  ê°ì†Œ ìŠ¤í…
TARGET_UPDATE         = 1000    # íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì£¼ê¸° (ìŠ¤í…)
MEMORY_CAPACITY       = 100000
NUM_EPISODES          = 1000
MAX_STEPS_PER_EPISODE = 1000
LEARNING_RATE         = 5e-5
resize = (84, 84)
env = CarRacingDiscrete(resize=resize)
env.reset(seed=42)

# ì´ˆê¸° í™”ë©´ìœ¼ë¡œë¶€í„° height, width ì–»ê¸° (í‘ë°± 1ì±„ë„ â†’ (84,84))
init_screen, _ = env.reset()
screen_height, screen_width = init_screen.shape  # (84, 84)


# 1) ì›ë˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¨¼ì € ìƒì„±í•˜ê³ 
_base_policy_net = DQN(resize[0], resize[1], env.action_space.n)
_base_target_net = DQN(resize[0], resize[1], env.action_space.n)

# 2) ë©€í‹°-GPU ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ DataParallelë¡œ ë˜í•‘
policy_net = nn.DataParallel(_base_policy_net).to(device)
target_net = nn.DataParallel(_base_target_net).to(device)

# 3) Policy ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°ë¥¼ Target ë„¤íŠ¸ì›Œí¬ì— ë³µì‚¬
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

# 4) ì˜µí‹°ë§ˆì´ì €: ì´ì œ policy_net.parameters()ëŠ” DataParallelì´ ë˜í•‘í•œ íŒŒë¼ë¯¸í„° ì „ë¶€ë¥¼ ê°€ë¦¬í‚´
optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)

memory = ReplayMemory(MEMORY_CAPACITY)
steps_done = 0

# TensorBoard writer
current_time = time.strftime('%Y%m%d_%H%M%S')
writer = SummaryWriter(log_dir=f"runs/dqn_lr_{LEARNING_RATE}_b{BATCH_SIZE}_EPS_DECAY_{EPS_DECAY}_e{NUM_EPISODES}_s{MAX_STEPS_PER_EPISODE}_{current_time}/teacher")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. FrameStack í´ë˜ìŠ¤ (ìƒíƒœ ì „ì²˜ë¦¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class FrameStack:
    def __init__(self, k):
        self.k = k
        self.frames = deque([], maxlen=k)

    def reset(self, obs):
        for _ in range(self.k):
            self.frames.append(obs)
        return self._get_state()

    def step(self, obs):
        self.frames.append(obs)
        return self._get_state()

    def _get_state(self):
        return np.stack(self.frames, axis=0)  # (k, H, W)

frame_stack = FrameStack(4)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. í–‰ë™ ì„ íƒ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def select_action(state):
    global steps_done
    sample = random.random()
    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1

    if sample > eps_threshold:
        with torch.no_grad():
            # state: [1, 4, H, W], GPU í…ì„œ. DataParallel wrapperê°€ ë‚´ë¶€ì ìœ¼ë¡œ ë°°ì¹˜ ì°¨ì›ì„ GPUë³„ë¡œ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬.
            action_values = policy_net(state)  # [1, action_dim]
            return action_values.max(1)[1].view(1, 1)  # í–‰ë™ ì¸ë±ìŠ¤ (GPU í…ì„œ)
    else:
        # ëœë¤ í–‰ë™ (GPU í…ì„œ)
        return torch.tensor([[random.randrange(env.action_space.n)]], device=device, dtype=torch.long)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. ëª¨ë¸ ìµœì í™” í•¨ìˆ˜ (í•™ìŠµ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def optimize_model():
    if len(memory) < BATCH_SIZE:
        return None

    transitions = memory.sample(BATCH_SIZE)
    batch = Transition(*zip(*transitions))

    # âœ” next_stateê°€ Noneì´ ì•„ë‹Œ ê²ƒë§Œ ê³¨ë¼ë‚¼ ë§ˆìŠ¤í¬
    non_final_mask = torch.tensor(
        tuple(map(lambda s: s is not None, batch.next_state)),
        device=device, dtype=torch.bool
    )
    # âœ” Noneì´ ì•„ë‹Œ ë‹¤ìŒ ìƒíƒœë“¤ë§Œ ëª¨ì•„ì„œ 2D í…ì„œë¡œ ë³€í™˜
    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])

    # âœ” ë°°ì¹˜ ë‚´ state, action, reward ëª¨ë‘ GPU í…ì„œ
    state_batch  = torch.cat(batch.state)    # [BATCH_SIZE, 4, H, W] on GPU
    action_batch = torch.cat(batch.action)   # [BATCH_SIZE, 1] on GPU
    reward_batch = torch.cat(batch.reward)   # [BATCH_SIZE, 1] on GPU

    # 1) í˜„ì¬ ì •ì±… ë„¤íŠ¸ì›Œí¬ê°€ ì˜ˆì¸¡í•œ Qê°’ ì¤‘ í•´ë‹¹ í–‰ë™ì˜ ê°’ë§Œ ê³¨ë¼
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # 2) íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ë¡œë¶€í„° ë‹¤ìŒ ìƒíƒœì—ì„œì˜ ìµœëŒ€ Qê°’ ê³„ì‚°
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    with torch.no_grad():
        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch.squeeze()

    # 3) Huber Loss (Smooth L1) ê³„ì‚°
    loss = F.smooth_l1_loss(state_action_values.squeeze(), expected_state_action_values)

    # 4) ì—­ì „íŒŒ ë° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 1)  # ê¸°ìš¸ê¸° í´ë¦¬í•‘
    optimizer.step()

    return loss.item()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9. ì‹¤ì œ í•™ìŠµ ë£¨í”„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_rewards     = []
all_mean_losses = []

for i_episode in range(1, NUM_EPISODES + 1):
    # 1) ì—í”¼ì†Œë“œ ì´ˆê¸° ìƒíƒœ ì¤€ë¹„
    obs, _ = env.reset()
    state = frame_stack.reset(obs)  # [4, 84, 84], numpy
    state = torch.from_numpy(state).unsqueeze(0).to(device, dtype=torch.float32)  # [1,4,84,84] on GPU

    total_reward   = 0.0
    episode_losses = []

    for t in range(MAX_STEPS_PER_EPISODE):
        # 2) í–‰ë™ ì„ íƒ (Îµ-greedy)
        action = select_action(state)

        # 3) í™˜ê²½ì— í–‰ë™ ì ìš©
        obs, reward, terminated, truncated, _ = env.step(action.item())
        total_reward += reward

        # 4) ë³´ìƒì„ GPU í…ì„œë¡œ ë³€í™˜
        reward_tensor = torch.tensor([[reward]], device=device, dtype=torch.float32)

        # 5) ë‹¤ìŒ ìƒíƒœ (ì¢…ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´)
        next_state = None
        if not terminated and not truncated:
            next_state_np = frame_stack.step(obs)  # [4,84,84], numpy
            next_state = torch.from_numpy(next_state_np).unsqueeze(0).to(device, dtype=torch.float32)

        # 6) ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì— ì €ì¥ (ëª¨ë‘ GPU í…ì„œ)
        memory.push(state, action, next_state, reward_tensor)

        # 7) ìƒíƒœ ì—…ë°ì´íŠ¸
        state = next_state if next_state is not None else state

        # 8) ë°°ì¹˜ ìƒ˜í”Œë§ & ëª¨ë¸ ìµœì í™” (GPU)
        loss_val = optimize_model()
        if loss_val is not None:
            episode_losses.append(loss_val)

        # 9) ì¼ì • ìŠ¤í…ë§ˆë‹¤ íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        if steps_done % TARGET_UPDATE == 0:
            target_net.load_state_dict(policy_net.state_dict())

        if terminated or truncated:
            break

    # â”€â”€ ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ê¸°ë¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_rewards.append(total_reward)
    if episode_losses:
        mean_loss = np.mean(episode_losses)
    else:
        mean_loss = 0.0
    all_mean_losses.append(mean_loss)

    # TensorBoard ê¸°ë¡
    writer.add_scalar("Episode/Reward", total_reward, i_episode)
    writer.add_scalar("Episode/Loss", mean_loss, i_episode)
    writer.add_scalar("Episode/Epsilon",
                      EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY),
                      i_episode)
    writer.add_scalar("Episode/Steps", t+1, i_episode)

    print(f"Episode {i_episode}: total reward = {total_reward:.1f}, steps = {t+1}, mean loss = {mean_loss:.4f}")

    # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì„ íƒ)
    if i_episode % 50 == 0:
        torch.save({
            'episode': i_episode,
            'model_state_dict': policy_net.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'memory': memory,
            'steps_done': steps_done
        }, f"carracing_dqn_5e-5_checkpoint_ep{i_episode}.pt")

print("Training complete")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 10. ìµœì¢… ëª¨ë¸ ì €ì¥ ë° ì¢…ë£Œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final_checkpoint = {
    'model': policy_net,  # DataParallel ë˜í•‘ëœ ëª¨ë¸ ì €ì¥
    'optimizer_state_dict': optimizer.state_dict(),
    'memory': memory,
    'steps_done': steps_done,
    'episode': NUM_EPISODES
}
torch.save(final_checkpoint, "carracing_dqn_final_model_lr_{LEARNING_RATE}_b{BATCH_SIZE}_EPS_DECAY_{EPS_DECAY}_e{NUM_EPISODES}_s{MAX_STEPS_PER_EPISODE}.pt")

writer.close()
env.close()


my_plot(all_rewards, all_mean_losses,
            EPS_DECAY, NUM_EPISODES, MAX_STEPS_PER_EPISODE,
            LEARNING_RATE, BATCH_SIZE)
