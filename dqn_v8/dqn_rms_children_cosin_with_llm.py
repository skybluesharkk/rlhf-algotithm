import gymnasium as gym
import math, random, time
import numpy as np
import re
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T
from torch.cuda.amp import GradScaler
from torch.utils.tensorboard import SummaryWriter
from collections import namedtuple, deque
from PIL import Image


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ÏÑ§Ï†ï
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SEED              = 42
WARMUP_EPISODES   = 50        # ÏõåÎ∞çÏóÖ Í∏∞Í∞Ñ Îã®Ï∂ï (100 -> 50)
NUM_EPISODES      = 1000
MAX_STEPS         = 1000
BATCH_SIZE        = 128
GAMMA             = 0.99
EPS_START         = 1.0
EPS_END           = 0.05
EPS_DECAY         = 50_000          
TARGET_UPDATE     = 1000
MEMORY_CAPACITY   = 100_000
FEEDBACK_WEIGHT   = 0.5            
LR                = 5e-5
RESIZE            = (84, 84)

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.benchmark = True
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Ï†ÑÏó≠Ïóê LLM ÌååÏù¥ÌîÑÎùºÏù∏ Ìïú Î≤àÎßå ÏÉùÏÑ±
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteriaList

MODEL_NAME = "google/gemma-7b-it"
_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
_model     = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, device_map={"":0}, torch_dtype=torch.float16
)
_pipe = pipeline(
    "text-generation",
    model=_model,
    tokenizer=_tokenizer,
    return_full_text=False
)
# Ï§ÑÎ∞îÍøà Ïãú Ï§ëÎã®
from transformers import StoppingCriteria
class _StopAtNL(StoppingCriteria):
    def __init__(self, tok): self.id = tok.encode("\n")[0]
    def __call__(self, ids, scores, **kwargs):
        return ids[0, -1] == self.id
_stopper = StoppingCriteriaList([_StopAtNL(_tokenizer)])



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1) ÌôòÍ≤Ω ÎûòÌçº
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class CarRacingDiscrete(gym.Wrapper):
    ACTIONS = [
        np.array([-1.0, 0.0, 0.0], np.float32),
        np.array([ 1.0, 0.0, 0.0], np.float32),
        np.array([ 0.0, 1.0, 0.0], np.float32),
        np.array([ 0.0, 0.0, 0.8], np.float32),
        np.array([-1.0, 1.0, 0.0], np.float32),
        np.array([ 1.0, 1.0, 0.0], np.float32),
        np.array([ 0.0, 0.0, 0.0], np.float32),
    ]
    def __init__(self, resize=RESIZE):
        env = gym.make("CarRacing-v3", render_mode="rgb_array")
        super().__init__(env)
        self.action_space      = gym.spaces.Discrete(len(self.ACTIONS))
        self.observation_space = gym.spaces.Box(0,255,(resize[0],resize[1],1),dtype=np.uint8)
        self.tf = T.Compose([
            T.ToPILImage(),
            T.Grayscale(1),
            T.Resize(resize, T.InterpolationMode.BILINEAR),
            T.ToTensor(),
        ])
    def _proc(self, frame):
        x = self.tf(frame).squeeze(0)
        return (x*255).byte().numpy()
    def step(self, a):
        obs,r,term,trunc,info = self.env.step(self.ACTIONS[a])
        return self._proc(obs), r, term, trunc, info
    def reset(self, **kw):
        obs,info = self.env.reset(**kw)
        return self._proc(obs), info

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2) FrameStack
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class FrameStack:
    def __init__(self, k): self.k, self.buf = k, deque([], maxlen=k)
    def reset(self, obs):
        for _ in range(self.k): self.buf.append(obs)
        return np.stack(self.buf, 0)
    def step(self, obs):
        self.buf.append(obs)
        return np.stack(self.buf, 0)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3) DQN ÎÑ§Ìä∏ÏõåÌÅ¨
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class DQN(nn.Module):
    def __init__(self, h, w, outputs):
        super().__init__()
        self.conv1 = nn.Conv2d(4,32,8,4)
        self.conv2 = nn.Conv2d(32,64,4,2)
        self.conv3 = nn.Conv2d(64,64,3,1)
        def co(o,k,s): return (o-(k-1)-1)//s+1
        cw = co(co(co(w,8,4),4,2),3,1)
        ch = co(co(co(h,8,4),4,2),3,1)
        lin = cw*ch*64
        self.fc   = nn.Linear(lin,512)
        self.head = nn.Linear(512,outputs)
    def forward(self, x):
        x = x/255.0
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = torch.flatten(x,1)
        x = F.relu(self.fc(x))
        return self.head(x)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4) ReplayMemory
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Transition = namedtuple('Transition',
    ('state','action','next_state','reward','time','feedback'),
    defaults=[0,0]
)
class ReplayMemory:
    def __init__(self, cap): self.mem = deque([], maxlen=cap)
    def push(self, *args, cnt=1):
        tr = Transition(*args)
        for _ in range(cnt): self.mem.append(tr)
    def sample(self, b): return random.sample(self.mem, b)
    def __len__(self): return len(self.mem)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 5) Í∞úÏÑ†Îêú Î≥¥Ï°∞ Ìï®ÏàòÎì§
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def action_distance(i, j, actions):
    actions = [
        "[-1,0,0]", "[1,0,0]", "[0,1,0]", "[0,0,0.8]",
        "[-1,1,0]", "[1,1,0]", "[0,0,0]"
    ]
    prompt = (
        "Task: Calculate car action similarity with these STRICT rules:\n"
        "\n"
        "Action meanings:\n"
        "- steer: -1=left, +1=right, 0=straight\n"
        "- throttle: acceleration (higher = faster)\n"
        "- brake: deceleration (higher = stronger braking)\n"
        "Examples: \n"
        "scores could be one of this numbers that between 0 and 1. ex) 0.00,0.25,0.50,0.75,1.00\n"
        " ex) a=[-1,0,0], b=[1,0,0] -> 1.00, a=[0,0,0], b=[0,0,0] -> 0.00\n"
        "There is no score larger than 1.\n"
        "\n"
        f"a={actions[i]}\n"
        f"b={actions[j]}\n"
        "\n"
        "Check rules first, then calculate. Output only decimal number:\n"
        "score: "
    )

    out =_pipe(
        prompt,
        max_new_tokens=5,         
        do_sample=True,
        temperature=0.9,          
        top_p=0.9,                 
        top_k=40,                 
        repetition_penalty=1.05,  
        stopping_criteria=_stopper
    )[0]["generated_text"]

    m = re.search(r"[-+]?\d*\.?\d+", out)
    print("m :",m)
    if not m:
        return 0.5
    return float(m.group())


def to_feedback(dist):
    """Îçî ÏóÑÍ≤©Ìïú ÌîºÎìúÎ∞± ÏûÑÍ≥ÑÍ∞í"""
    if dist <= 0.2:     return  1    # Îß§Ïö∞ Ïú†ÏÇ¨ (Îçî ÏóÑÍ≤©)
    elif dist <= 0.5:   return  0    # Î≥¥ÌÜµ
    else:               return -1    # Îß§Ïö∞ Îã§Î¶Ñ

def cnt_for_fb(fb, ep, tot_remaining_episodes):
    """negative feedbackÎèÑ ÌôúÏö©ÌïòÎäî Í∞úÏÑ†Îêú Ïπ¥Ïö¥Ìä∏ Ï†ÑÎûµ"""
    if tot_remaining_episodes <= 0:
        return 1  # Í∏∞Î≥∏Í∞í
    
    # ÌïôÏäµ ÏßÑÌñâÎèÑÏóê Îî∞Î•∏ ÎèôÏ†Å Ï°∞Ï†ï
    progress = min(1.0, ep / tot_remaining_episodes)
    
    if fb == 1:      # positive feedback
        base_cnt = 4
        return max(1, int(base_cnt * (1 - progress * 0.5)))  # Ï†êÏßÑÏ†Å Í∞êÏÜå
    elif fb == 0:    # neutral feedback  
        return 1
    else:            # negative feedback 
        base_cnt = 3
        return max(1, int(base_cnt * (1 - progress * 0.3)))  # negativeÎèÑ ÌïôÏäµÏóê ÌôúÏö©

def epsilon(step):
    return EPS_END + (EPS_START-EPS_END)*math.exp(-step/EPS_DECAY)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 6) Ï¥àÍ∏∞Ìôî
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
env         = CarRacingDiscrete()
frame_stack = FrameStack(4)


teacher_net = DQN(*RESIZE, env.action_space.n).to(device)
teacher_net.load_state_dict(torch.load("dqn_weights_only.pth", map_location=device))
teacher_net.eval()

student_net = DQN(*RESIZE, env.action_space.n).to(device)
target_net  = DQN(*RESIZE, env.action_space.n).to(device)
target_net.load_state_dict(student_net.state_dict()); target_net.eval()

optimizer   = optim.RMSprop(student_net.parameters(), lr=LR)

# CosineAnnealingLR Ïä§ÏºÄÏ§ÑÎü¨ Ï∂îÍ∞Ä  ‚òÖ
from torch.optim.lr_scheduler import CosineAnnealingLR
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=NUM_EPISODES,
    eta_min=1e-6
)

memory      = ReplayMemory(MEMORY_CAPACITY)
scaler      = GradScaler(enabled=torch.cuda.is_available())
writer      = SummaryWriter(log_dir=f"runs/cosin_improved_exp_{int(time.time())}")

steps_done  = 0

# ÎîîÎ≤ÑÍπÖÏö© ÌÜµÍ≥Ñ Î≥ÄÏàò
feedback_stats = {'positive': 0, 'neutral': 0, 'negative': 0}
distance_history = []

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 7) optimize_model
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def optimize_model():
    if len(memory) < BATCH_SIZE: return None
    batch = Transition(*zip(*memory.sample(BATCH_SIZE)))
    non_mask = torch.tensor([s is not None for s in batch.next_state], device=device, dtype=torch.bool)
    non_next = torch.cat([s for s in batch.next_state if s is not None]) if non_mask.any() else torch.empty(0,device=device)

    S  = torch.cat(batch.state).to(device)
    A  = torch.cat(batch.action).to(device)
    R  = torch.cat(batch.reward).to(device)
    FB = torch.tensor(batch.feedback, device=device).float()
    
    # CarRacing ÌôòÍ≤ΩÏùò Î¶¨ÏõåÎìú Ïä§ÏºÄÏùº Í≥†Î†§ÌïòÏó¨ ÌîºÎìúÎ∞± Ï°∞Ï†ï
    scaled_feedback = FEEDBACK_WEIGHT * FB
    total_r = R + scaled_feedback

    Qsa = student_net(S).gather(1, A)
    with torch.no_grad():
        next_v = torch.zeros(BATCH_SIZE, device=device)
        if non_next.numel()>0:
            next_v[non_mask] = target_net(non_next).max(1)[0]
    target = next_v*GAMMA + total_r.squeeze()
    loss = F.smooth_l1_loss(Qsa.squeeze(), target)

    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_value_(student_net.parameters(), 1)
    optimizer.step()
    return loss.item()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 8) Í∞úÏÑ†Îêú ÌïôÏäµ Î£®ÌîÑ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
reward_hist, loss_hist = [], []

for ep in range(1, NUM_EPISODES+1):
    obs,_  = env.reset()
    state  = torch.from_numpy(frame_stack.reset(obs)).unsqueeze(0).to(device).float()
    ep_r, ep_losses = 0.0, []
    
    # ÏóêÌîºÏÜåÎìúÎ≥Ñ ÌîºÎìúÎ∞± ÌÜµÍ≥Ñ Ï¥àÍ∏∞Ìôî
    ep_feedback_stats = {'positive': 0, 'neutral': 0, 'negative': 0}
    ep_distances = []

    for t in range(MAX_STEPS):
        if random.random() > epsilon(steps_done):
            with torch.no_grad(): act = student_net(state).argmax(1).view(1,1)
        else:
            act = torch.tensor([[random.randrange(env.action_space.n)]], device=device)
        steps_done += 1 
        
        # Í∞úÏÑ†Îêú feedback Î©îÏª§ÎãàÏ¶ò
        if ep > WARMUP_EPISODES:
            with torch.no_grad():
                ta = teacher_net(state).argmax(1).view(1,1)
            
            if act.item() != ta.item():
                dist = action_distance(act.item(), ta.item(), env.ACTIONS)
                print("dist: ",dist)
                fb = to_feedback(dist)
                ep_distances.append(dist)
            else:
                dist, fb = 0.0, 1  # ÎèôÏùºÌïú Ïï°ÏÖòÏùÄ positive feedback
                ep_distances.append(0.0)
            
            # ÌîºÎìúÎ∞± ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            if fb == 1:
                ep_feedback_stats['positive'] += 1
            elif fb == 0:
                ep_feedback_stats['neutral'] += 1
            else:
                ep_feedback_stats['negative'] += 1
            
            cnt = cnt_for_fb(fb, ep - WARMUP_EPISODES, NUM_EPISODES - WARMUP_EPISODES)
        else:
            dist, fb, cnt = 0.0, 0, 1

        # env step
        obs,r,term,trunc,_ = env.step(act.item())
        ep_r += r
        rt = torch.tensor([[r]], device=device)

        next_state = None
        if not (term or trunc):
            ns_np = frame_stack.step(obs)
            next_state = torch.from_numpy(ns_np).unsqueeze(0).to(device).float()

        memory.push(state, act, next_state, rt, time.time(), fb, cnt=cnt)
        state = next_state if next_state is not None else state

        l = optimize_model()
        if l is not None: ep_losses.append(l)

        if steps_done % TARGET_UPDATE == 0:
            target_net.load_state_dict(student_net.state_dict())
        if term or trunc: break

    # ÏóêÌîºÏÜåÎìú ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
    reward_hist.append(ep_r)
    loss_hist.append(np.mean(ep_losses) if ep_losses else 0.0)
    
    # TensorBoard Î°úÍπÖ (Í∞úÏÑ†Îêú ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥ Ìè¨Ìï®)
    writer.add_scalar("Episode/Reward", ep_r, ep)
    writer.add_scalar("Episode/Loss", np.mean(ep_losses) if ep_losses else 0.0, ep)
    writer.add_scalar("Episode/Learning_Rate", scheduler.get_last_lr()[0], ep)
    writer.add_scalar("Episode/Epsilon", epsilon(steps_done), ep)
    writer.add_scalar("Episode/Steps", t+1, ep)
    
    # ÌîºÎìúÎ∞± ÌÜµÍ≥Ñ Î°úÍπÖ
    if ep > WARMUP_EPISODES:
        writer.add_scalar("Feedback/Positive_Count", ep_feedback_stats['positive'], ep)
        writer.add_scalar("Feedback/Neutral_Count", ep_feedback_stats['neutral'], ep)
        writer.add_scalar("Feedback/Negative_Count", ep_feedback_stats['negative'], ep)
        
        if ep_distances:
            avg_distance = np.mean(ep_distances)
            writer.add_scalar("Feedback/Average_Distance", avg_distance, ep)
            distance_history.extend(ep_distances)
        
        # ÌîºÎìúÎ∞± ÎπÑÏú®
        total_fb = sum(ep_feedback_stats.values())
        if total_fb > 0:
            writer.add_scalar("Feedback/Positive_Ratio", ep_feedback_stats['positive']/total_fb, ep)
            writer.add_scalar("Feedback/Negative_Ratio", ep_feedback_stats['negative']/total_fb, ep)

    # Ïä§ÏºÄÏ§ÑÎü¨ Ïä§ÌÖù  ‚òÖ
    scheduler.step()

    # Í∞úÏÑ†Îêú Ï∂úÎ†• (ÌîºÎìúÎ∞± Ï†ïÎ≥¥ Ìè¨Ìï®)
    if ep > WARMUP_EPISODES and ep_distances:
        avg_dist = np.mean(ep_distances)
        total_fb = sum(ep_feedback_stats.values())
        fb_summary = f"FB(+{ep_feedback_stats['positive']}/0{ep_feedback_stats['neutral']}/-{ep_feedback_stats['negative']}) Dist:{avg_dist:.3f}"
        print(f"Ep {ep:4d} | Reward {ep_r:7.1f} | Loss {np.mean(ep_losses):7.4f} | {fb_summary}")
    else:
        print(f"Ep {ep:4d} | Reward {ep_r:7.1f} | Loss {np.mean(ep_losses):7.4f} | WARMUP")

    # Ï£ºÍ∏∞Ï†Å Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•
    if ep % 100 == 0:
        torch.save({
            'episode': ep,
            'student_state_dict': student_net.state_dict(),
            'target_state_dict': target_net.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'memory': memory,
            'steps_done': steps_done,
            'reward_history': reward_hist,
            'loss_history': loss_hist,
            'feedback_stats': feedback_stats
        }, f"cosin_student_checkpoint_ep{ep}.pt")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 9) ÎßàÎ¨¥Î¶¨ Î∞è Î∂ÑÏÑù
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# ÏµúÏ¢Ö ÌÜµÍ≥Ñ Ï∂úÎ†•
if distance_history:
    print(f"\nüìä Final Statistics:")
    print(f"   Average action distance: {np.mean(distance_history):.4f}")
    print(f"   Distance std: {np.std(distance_history):.4f}")
    print(f"   Min/Max distance: {np.min(distance_history):.4f}/{np.max(distance_history):.4f}")

total_feedback = sum(feedback_stats.values())
if total_feedback > 0:
    print(f"   Total feedback events: {total_feedback}")
    print(f"   Positive: {feedback_stats['positive']} ({feedback_stats['positive']/total_feedback*100:.1f}%)")
    print(f"   Neutral: {feedback_stats['neutral']} ({feedback_stats['neutral']/total_feedback*100:.1f}%)")
    print(f"   Negative: {feedback_stats['negative']} ({feedback_stats['negative']/total_feedback*100:.1f}%)")

# ÏµúÏ¢Ö Î™®Îç∏ Ï†ÄÏû•
final_save_dict = {
    'student_net': student_net.state_dict(),
    'target_net': target_net.state_dict(),
    'optimizer': optimizer.state_dict(),
    'scheduler': scheduler.state_dict(),
    'hyperparameters': {
        'WARMUP_EPISODES': WARMUP_EPISODES,
        'FEEDBACK_WEIGHT': FEEDBACK_WEIGHT,
        'EPS_DECAY': EPS_DECAY,
        'LR': LR,
        'BATCH_SIZE': BATCH_SIZE,
        'MAX_ACTION_DISTANCE': 1.0
    },
    'final_stats': {
        'reward_history': reward_hist,
        'loss_history': loss_hist,
        'distance_history': distance_history,
        'feedback_stats': feedback_stats
    }
}

torch.save(final_save_dict, "student_final_improved_cosin.pt")
writer.close(); env.close()